{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d665d-8afc-410a-8b41-960731e9b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import hashlib\n",
    "import importlib\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca622c39-5bed-41ff-8fb4-a8833c2b7f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ssdeep\n",
    "import torch\n",
    "import transformers\n",
    "from charset_normalizer import detect as cdetect\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from transformers import (AutoConfig, AutoModelForSequenceClassification,\n",
    "                          AutoTokenizer, EarlyStoppingCallback,\n",
    "                          TextClassificationPipeline, Trainer,\n",
    "                          TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c8f374-1078-4e8f-aad7-450d95846f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not transformers.is_torch_available():\n",
    "    raise RuntimeError(\"Torch is not available, make sure your Python env and dependencies are set\")\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"Cuda is not available, please retry on a Cuda capable device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2b12ff-b66b-40be-819a-4185e438b28e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a7cbd7-bfa6-4515-9717-6b62486e1720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_MINUSONE_AVAILABLE = importlib.util.find_spec('pyminusone') is not None\n",
    "if _MINUSONE_AVAILABLE:\n",
    "    import pyminusone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4565f532-4eee-4e18-8066-69f827b68c8b",
   "metadata": {},
   "source": [
    "## Required variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5299504-7cad-4e9c-b5e4-c87871c3b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets of legit/malicious pwsh scripts\n",
    "GOODWARES_DIR: str = \"\"\n",
    "assert os.path.exists(GOODWARES_DIR), \"Empty goodwares folder\"\n",
    "MALWARES_DIR: str = \"\"\n",
    "assert os.path.exists(MALWARES_DIR), \"Empty malwares folder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45579b6-4780-46e8-82b1-660033005e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Folder containing model bin and tokenizer files\n",
    "MODEL_FOLDER = \"\"\n",
    "assert os.path.exists(MODEL_FOLDER), \"Empty model folder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfd9763-e399-42c8-b7f7-70dd2b898ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model training output folder\n",
    "OUT_FOLDER = \"\"\n",
    "assert OUT_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f26a5ed-64ce-4556-a439-f95dc77d7c3f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c198ee8c-3313-4eb0-9818-1310f86b9e03",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727350d3-8bb9-4f91-ae3a-03240dd50278",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d0f519-b1ee-483d-9e29-fa257fa8a96a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_TRUNCATE_FILE_SIZE = 4096\n",
    "_CMT_REGEX = re.compile(r\"\\'[^\\']*\\'|\\\"[^\\\"]*\\\"|(#.*$|<#[\\s\\S]*?#>)\", flags=re.IGNORECASE | re.MULTILINE)\n",
    "_SSDEEP_THRESHOLD = 10\n",
    "_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d64c9d-af38-4721-9236-5a471254ce85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PreprocessedFile(NamedTuple):\n",
    "    sha256: str\n",
    "    ssdeep: str\n",
    "    content: str\n",
    "    encoding: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6f5b14-7150-4aa4-8015-235446972e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_and_decode(filepath: str) -> tuple[str|None, str|None]:\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as file_obj:\n",
    "            datab = file_obj.read()\n",
    "            enc = cdetect(datab)[\"encoding\"]\n",
    "            if enc is not None:\n",
    "                file_content = datab.decode(encoding=enc)\n",
    "            else:\n",
    "                # fallback on utf8 if detection failed\n",
    "                file_content = datab.decode(encoding=\"utf-8\")\n",
    "    except (OSError, UnicodeDecodeError) as ex:\n",
    "        print(f\"Unable to decode file {filepath}, error: {ex}\")\n",
    "        return None, None\n",
    "    return file_content, enc\n",
    "\n",
    "def _replace_callback(m: re.Match) -> str:\n",
    "    if m.group(1):\n",
    "        return \"\"\n",
    "    return m.group(0)\n",
    "\n",
    "def normalize_text(content: str) -> str | None:\n",
    "    try:\n",
    "        content = _CMT_REGEX.sub(_replace_callback, content)\n",
    "        while len(content) > 0:\n",
    "            if content[0] in string.whitespace:\n",
    "                content = content[1:]\n",
    "            else:\n",
    "                break\n",
    "    except Exception as ex:\n",
    "        print(f\"Unable to normalize file, error: {ex}\")\n",
    "        return None\n",
    "    return content[:_TRUNCATE_FILE_SIZE]\n",
    "\n",
    "def hash_file(filepath: str, enc: str) -> tuple[str|None, str|None]:\n",
    "    with open(filepath, 'rb') as f:\n",
    "        datab = f.read()\n",
    "        try :\n",
    "            sha256_hash = hashlib.sha256(datab).hexdigest()\n",
    "            ssdeep_hash = ssdeep.hash(datab, enc)\n",
    "        except Exception as ex:\n",
    "            print(f\"Couldn't hash {filepath}: {ex}\")\n",
    "            return None, None\n",
    "    return sha256_hash, ssdeep_hash\n",
    "\n",
    "def deobfuscate_with_minusone(content: str) -> str|None:\n",
    "    try:\n",
    "        content = pyminusone.deobfuscate_powershell(content)\n",
    "    except BaseException as ex:\n",
    "        print(f\"Unable to deobfuscate: {ex}\")\n",
    "        return None\n",
    "    return content\n",
    "\n",
    "def preprocess_file(filepath: str) -> PreprocessedFile | None:\n",
    "    content, encoding = read_and_decode(filepath)\n",
    "    if content is None:\n",
    "        return None\n",
    "    content = normalize_text(content)\n",
    "    if content is None or len(content) == 0:\n",
    "        return None\n",
    "    if _MINUSONE_AVAILABLE:\n",
    "        content = deobfuscate_with_minusone(content)\n",
    "        if content is None:\n",
    "            return None\n",
    "    sha256_hash, ssdeep_hash = hash_file(filepath, encoding)\n",
    "    if sha256_hash is None or ssdeep_hash is None:\n",
    "        return None\n",
    "    return PreprocessedFile(sha256_hash, ssdeep_hash, content, encoding)\n",
    "\n",
    "def preprocess_folder(folder: str | os.PathLike) -> list[PreprocessedFile]:\n",
    "    with ProcessPoolExecutor(max_workers=os.cpu_count()//2) as executor:\n",
    "        file_paths = sorted([os.path.join(root, file) for root, _, files in os.walk(folder) for file in files])\n",
    "        file_array = []\n",
    "        with tqdm(total=len(file_paths), desc=\"Processing files\", unit=\"file\") as pbar:\n",
    "            for r in executor.map(preprocess_file, file_paths):\n",
    "                if r:\n",
    "                    file_array.append(r)\n",
    "                pbar.update(1)\n",
    "    return file_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00334fcc-0ade-4097-aab7-99827c76a411",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "gw = preprocess_folder(GOODWARES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6de85a-c196-42aa-8506-101d0c8a4343",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "mw = preprocess_folder(MALWARES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d757b850-5968-4805-a3fa-a70fa9918535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Got {len(gw)} goodwares, {len(mw)} malwares\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc57b63a-1047-459f-8529-83897831b231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not gw or not mw:\n",
    "    raise RuntimeError(\"Left with no files after preprocessing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdcf47b-2858-4905-8ba5-a44c8f7ec9c4",
   "metadata": {},
   "source": [
    "### Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c91e9c1-54cd-40a1-a615-5eff97f92124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a file is a duplicate based on ssdeep hash\n",
    "def deduplicate_files(preprocessed_files: list[PreprocessedFile], threshold: int = _SSDEEP_THRESHOLD):\n",
    "    unique_files = set()\n",
    "\n",
    "    def is_duplicate(current_hash: str):\n",
    "        for f in unique_files:\n",
    "            proximity = ssdeep.compare(current_hash, f.ssdeep)\n",
    "            if proximity > threshold:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    with tqdm(total=len(preprocessed_files), desc=\"Deduplicating files\", unit=\"file\") as pbar:\n",
    "        for pfile in preprocessed_files:\n",
    "            if not is_duplicate(pfile.ssdeep):\n",
    "                unique_files.add(pfile)\n",
    "            pbar.update(1)\n",
    "\n",
    "    return unique_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff13a074-5d1f-4e12-b96c-242f6986cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_dedup = deduplicate_files(gw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8240e7b7-d569-4682-b9ee-c3cf2a2e99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mw_dedup = deduplicate_files(mw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f556a65-6d0b-4b7e-9dce-957e17251261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"After deduplication: {len(gw_dedup)} goodwares, {len(mw_dedup)} malwares, {len(gw_dedup)+len(mw_dedup)} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ee669f-723a-45d3-8713-f49478fbffa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not gw_dedup or not mw_dedup:\n",
    "    raise RuntimeError(\"No files left after deduplication!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a9e5a5-9805-438b-83d1-da0806a82247",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f0ee0e-bb3a-4790-a30d-e267cf143161",
   "metadata": {},
   "source": [
    "### Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df4785-aa19-492b-a567-1ec9bf713ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_array = []\n",
    "dataset_array += [{\"label\":0, \"text\":el.content, \"encoding\":el.encoding, \"hash\":el.sha256} for el in gw_dedup]\n",
    "dataset_array += [{\"label\":1, \"text\":el.content, \"encoding\":el.encoding, \"hash\":el.sha256} for el in mw_dedup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88575ad8-26ab-40f6-87e0-a63de4450b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shuffle dataset\n",
    "rd = random.Random(x=_SEED)\n",
    "rd.shuffle(dataset_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c256c-8f1d-4700-8871-b4571e70234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train, test, and validation datasets\n",
    "train_dataset = Dataset.from_list([item for item in dataset_array if item[\"hash\"][0] in set(\"0123456789ab\")])\n",
    "test_dataset = Dataset.from_list([item for item in dataset_array if item[\"hash\"][0] in set(\"cd\")])\n",
    "val_dataset = Dataset.from_list([item for item in dataset_array if item[\"hash\"][0] in set(\"ef\")])\n",
    "dataset = DatasetDict(\n",
    "    train=train_dataset,\n",
    "    validation=val_dataset,\n",
    "    test=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b29882-4e5f-406e-8e11-9721814a1390",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b87e3-f3fb-4b52-bd07-9bcd5b7020ee",
   "metadata": {},
   "source": [
    "### Pre-tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e017697-68ad-4a78-9c8d-230bb848b058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tokenizer(tokenizer):\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "    tokenizer.model_max_length = 1024\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbc5183-aa9e-4215-94c4-6a1d35a3d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_FOLDER)\n",
    "tokenizer = prepare_tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada0ff68-afae-4719-a6d3-25c9c7fd6120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(el):\n",
    "    return tokenizer(el[\"text\"], max_length=1024, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1778e59-6d7b-4c5b-b45f-78e62cb58218",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(tokenize, batched=False, num_proc=os.cpu_count()//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76be6f2-25a2-4e25-b143-9a7133ac13d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9e607e-ec75-4762-93c8-1f6825ccadd3",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112cbdac-3b89-47e2-888a-22c6bf37f264",
   "metadata": {},
   "source": [
    "### Init Trainer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc417f-3b64-432d-8039-389bf8fc9ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_name = \"demo\"\n",
    "train_batch_size = 32\n",
    "num_train_epochs = 4\n",
    "output_dir = os.path.join(OUT_FOLDER, run_name)\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d27c97-d47f-458d-8b1b-0a4fd63e68af",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=train_batch_size,\n",
    "    eval_steps=10,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    eval_accumulation_steps=1,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    run_name=run_name,\n",
    "    disable_tqdm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c376b9d7-c0d0-4bdc-b6be-f7c6da0ce858",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "configuration = AutoConfig.from_pretrained(MODEL_FOLDER)\n",
    "configuration.hidden_dropout_prob = 0.\n",
    "configuration.attention_probs_dropout_prob = 0.2\n",
    "configuration.classifier_dropout = 0.2\n",
    "configuration.num_labels = 2\n",
    "configuration.output_hidden_states = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409fae37-5c52-4e2b-823a-46cbbd42388b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_FOLDER,\n",
    "    config=configuration,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5a2ec4-d2aa-4624-bab5-750fd871348f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"bert\"):\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0678f487-4a03-418f-b573-311a85d943f7",
   "metadata": {},
   "source": [
    "### Init optimizer/scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a8e90-58ec-4f23-a13b-ef04441a2247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrained_parms = model.bert.parameters()\n",
    "pretrained_names = [f'bert.{k}' for (k, v) in model.bert.named_parameters()]\n",
    "classifier_parms = [v for k, v in model.named_parameters() if k not in pretrained_names]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    [\n",
    "        {\n",
    "            'params': pretrained_parms,\n",
    "            'lr': 2e-5,\n",
    "        },\n",
    "        {\n",
    "            'params': classifier_parms,\n",
    "            'lr': 1e-4,\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4239fd7f-ad13-486e-91c2-ea7bbe924899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr_scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=len(train_dataset) / train_batch_size * num_train_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3405baf-7a2a-409c-b1fd-ff6d325f5bc6",
   "metadata": {},
   "source": [
    "### Init metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504bf817-8bbc-4676-a27b-bc766554715b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyTrainer(Trainer):\n",
    "    def log(self, logs: dict[str, float]) -> None:\n",
    "        logs[\"LR*1e6\"] = self._get_learning_rate() * 1e6\n",
    "        super().log(logs)\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0.0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6025173b-ab34-4f94-a17c-9a40afdb1253",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260d2a80-b138-4bce-aab5-e57eaf9e23df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = MyTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    optimizers=(optimizer, lr_scheduler),\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(\n",
    "            early_stopping_patience=30, early_stopping_threshold=1e-3\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df1c17-03e8-4fd9-b098-2ed00fce0679",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652e5f2b-dd05-42b1-843b-b83f43a343d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Saving last checkpoint of the model\")\n",
    "model.save_pretrained(os.path.join(trainer.args.output_dir, \"final_checkpoint\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fb2cea-5599-4aba-b3df-093dc80b1e27",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e7131-2195-458c-8610-da767cdeb526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Evaluating on valid set...\")\n",
    "trainer.evaluate(tokenized_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2136b64f-a22f-4719-aa1d-d1f7df6a3b49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Evaluating on test set...\")\n",
    "trainer.evaluate(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65686471-cdfb-468f-a480-30b1e4977573",
   "metadata": {},
   "source": [
    "## Fp/Fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91f2fd-66c9-4e7e-a46f-41e36bbadfd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = TextClassificationPipeline(\n",
    "    model=model, tokenizer=tokenizer, top_k=None, max_length=1024, truncation=True, device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b436b1-6a77-4114-94c9-7dfa418589a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_dataset = concatenate_datasets(\n",
    "    [tokenized_datasets[\"test\"], tokenized_datasets[\"train\"], tokenized_datasets[\"validation\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2744a705-6c1b-4063-9aca-acb97104da97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = pipe(full_dataset[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79841ac2-f9a0-4970-a024-8b5b7abaefad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_mask = np.array([item[\"hash\"][0] in \"0123456789ab\" for item in full_dataset])\n",
    "test_mask = np.array([item[\"hash\"][0] in \"cd\" for item in full_dataset])\n",
    "valid_mask = np.array([item[\"hash\"][0] in \"ef\" for item in full_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b0dbf1-8bf7-4e7a-924c-a6de353f394b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = np.array([el[\"score\"] for p in preds for el in p if el[\"label\"] == \"LABEL_1\"])\n",
    "ground_truth = np.array(full_dataset[\"label\"], dtype=np.bool_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754a95bf-b1d3-42d3-9b23-b1dd278fb91c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fnr_from_fpr(predictions, ground_truth, fpr_threshold):\n",
    "    # Sort predictions in descending order\n",
    "    sorted_indices = np.argsort(predictions)[::-1]\n",
    "    sorted_predictions = predictions[sorted_indices]\n",
    "    sorted_ground_truth = ground_truth[sorted_indices]\n",
    "\n",
    "    # Calculate the cumulative sum of true positives and true negatives\n",
    "    cum_true_positives = np.cumsum(sorted_ground_truth)\n",
    "    cum_true_negatives = np.cumsum(1 - sorted_ground_truth)\n",
    "\n",
    "    # Calculate the total number of positives and negatives\n",
    "    total_positives = np.sum(sorted_ground_truth)\n",
    "    total_negatives = len(ground_truth) - total_positives\n",
    "\n",
    "    # Calculate the False Positive Rate (FPR) for each threshold\n",
    "    fpr = cum_true_negatives / total_negatives\n",
    "\n",
    "    # Find the index of the FPR closest to the specified threshold\n",
    "    fpr_index = np.argmin(np.abs(fpr - fpr_threshold))\n",
    "\n",
    "    # Calculate the corresponding False Negative Rate (FNR)\n",
    "    fnr = 1.0 - cum_true_positives[fpr_index] / total_positives\n",
    "\n",
    "    return fnr, sorted_predictions[fpr_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fcc2fc-d88e-4850-9713-74964be3b4bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Train thresholds\")\n",
    "for fpr_rate in [0.001, 0.005, 0.01, 0.02, 0.05]:\n",
    "    fnr, threshold = fnr_from_fpr(predictions[train_mask], ground_truth[train_mask], fpr_rate)\n",
    "    print(f\"False Negative Rate: at {fpr_rate*100:1.2f}% FP : {fnr*100:5.2f}% , threshold={threshold:1.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ac2be0-3471-4746-bcea-c0871b981657",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b0580-e34d-4185-8764-78640ffa552d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Valid thresholds\")\n",
    "for fpr_rate in [0.001, 0.005, 0.01, 0.02, 0.05]:\n",
    "    fnr, threshold = fnr_from_fpr(predictions[valid_mask], ground_truth[valid_mask], fpr_rate)\n",
    "    print(f\"False Negative Rate: at {fpr_rate*100:1.2f}% FP : {fnr*100:5.2f}% , threshold={threshold:1.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a9b6c1-d688-4674-a25e-a9fe58cf5e60",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b079fcd-6763-4bcd-bcc3-ef1b0458cd92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Test thresholds\")\n",
    "for fpr_rate in [0.001, 0.005, 0.01, 0.02, 0.05]:\n",
    "    fnr, threshold = fnr_from_fpr(predictions[test_mask], ground_truth[test_mask], fpr_rate)\n",
    "    print(f\"False Negative Rate: at {fpr_rate*100:1.2f}% FP : {fnr*100:5.2f}% , threshold={threshold:1.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92f3ec7-60f2-4965-b756-d3e402b325a9",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7067e118-10bb-417d-baec-759171c6c251",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Full dataset thresholds\")\n",
    "for fpr_rate in [0.001, 0.005, 0.01, 0.02, 0.05]:\n",
    "    fnr, threshold = fnr_from_fpr(predictions, ground_truth, fpr_rate)\n",
    "    print(f\"False Negative Rate: at {fpr_rate*100:1.2f}% FP : {fnr*100:5.2f}% , threshold={threshold:1.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e9af27-01db-4c22-95d8-c8549744d562",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ddd6b-65b0-4a79-8e2b-089fc0befaca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fpr_threshold = 0.005 # 0.5% FP\n",
    "fnr, thresh = fnr_from_fpr(predictions, ground_truth, fpr_threshold)\n",
    "predictions_bool = predictions > thresh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d742d315-1ea7-4335-8649-275cd5b40d0a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "FP = []\n",
    "FN = []\n",
    "for idx in np.where(predictions_bool != ground_truth)[0]:\n",
    "    if ground_truth[idx]:\n",
    "        FN.append((full_dataset[\"hash\"][idx], predictions[idx]))\n",
    "    else:\n",
    "        FP.append((full_dataset[\"hash\"][idx], predictions[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1eea36-b83d-4317-a349-a44686fbea50",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for h, s in sorted(FP, key=lambda x:x[1]):\n",
    "    print(\"FP: %s : %2.4f\" % (h, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1423da-abb8-461c-bdd8-304bd823c475",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for h, s in sorted(FN, key=lambda x:x[1]):\n",
    "   print(\"FN: %s : %2.4f\" % (h, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec39ee50-4a97-4732-966c-371f920100ba",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b448f3aa-c197-4864-acb0-d4b1fb8cff53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fgr-3.9-torch",
   "language": "python",
   "name": "fgr-3.9-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
